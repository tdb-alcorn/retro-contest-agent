{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from retro_contest.local import make\n",
    "import sys\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_q_agent import flatten, as_binary_array\n",
    "\n",
    "def pretrain(env, pretrain_length):\n",
    "    action_size = 2**env.action_space.n\n",
    "    memory = []\n",
    "    state = flatten(env.reset())\n",
    "    for i in range(pretrain_length):\n",
    "        action = np.random.randint(action_size)\n",
    "        next_state, reward, done, _ = env.step(as_binary_array(action, length=env.action_space.n))\n",
    "        next_state = flatten(next_state)\n",
    "        memory.append((state, action, reward, next_state, done))\n",
    "        state = next_state\n",
    "        if done:\n",
    "            state = flatten(env.reset())\n",
    "    return memory\n",
    "\n",
    "try:\n",
    "    env = make(game='SonicTheHedgehog-Genesis', state='LabyrinthZone.Act1')\n",
    "    pretrain_data = pretrain(env, 10000)\n",
    "finally:\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode =    9, Total Reward = 0.000377"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "from deep_q_agent import *\n",
    "\n",
    "train_episodes = 10            # max number of episodes to learn from\n",
    "gamma = 0.99                   # future reward discount\n",
    "\n",
    "# Exploration parameters\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "hidden_size = 64               # number of units in each Q-network hidden layer\n",
    "learning_rate = 0.0001         # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = 1000000            # memory capacity\n",
    "batch_size = 10000                # experience mini-batch size\n",
    "pretrain_length = batch_size   # number experiences to pretrain the memory\n",
    "\n",
    "tf.reset_default_graph()\n",
    "tf.logging.set_verbosity(tf.logging.WARN)\n",
    "\n",
    "try:\n",
    "    env = make(game='SonicTheHedgehog-Genesis', state='LabyrinthZone.Act1')\n",
    "\n",
    "    state_size = reduce(lambda x,y: x*y, env.observation_space.shape, 1)\n",
    "    action_size = 2**env.action_space.n\n",
    "\n",
    "    agent = DeepQAgent(\n",
    "        num_actions=action_size,\n",
    "        state_shape=state_size,\n",
    "        explore_start=explore_start,\n",
    "        explore_stop=explore_stop,\n",
    "        decay_rate=decay_rate,\n",
    "        hidden=hidden_size,\n",
    "        learning_rate=learning_rate,\n",
    "        memory_size=memory_size,\n",
    "        memory_prepop=pretrain_data,\n",
    "        batch_size=batch_size,\n",
    "        gamma=gamma,\n",
    "    )\n",
    "\n",
    "    # agent.pretrain(env, pretrain_length)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        agent.learn(sess, gamma=gamma)\n",
    "        \n",
    "        for episode in range(train_episodes):\n",
    "            state = flatten(env.reset())\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = agent.act(sess, state, train=True)\n",
    "                next_state, reward, done, info = env.step(as_binary_array(action, length=env.action_space.n))\n",
    "                next_state = flatten(next_state)\n",
    "                agent.step(sess, state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                if done:\n",
    "                    print(\"\\rEpisode = {:4d}, Total Reward = {:.3f}\".format(\n",
    "                        episode, agent.total_rewards[-1]), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "        saver.save(sess, \"checkpoints/deep-q-agent.ckpt\")\n",
    "finally:\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[40605.656,\n",
       " 25573.062,\n",
       " 8467.266,\n",
       " 7354.199,\n",
       " 6160.559,\n",
       " 4886.541,\n",
       " 4302.5034,\n",
       " 3316.8113,\n",
       " 2895.5972,\n",
       " 2948.9966]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-11300bd946e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m5000\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "x = (10000, 4096)\n",
    "c = tuple([True]*5000 + [False]*5000)\n",
    "y = np.zeros(x)\n",
    "y[c] = np.ones(x[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 1., 1., 1.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[4999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0, 1],\n",
       "        [2, 3]]), array([[4, 5],\n",
       "        [6, 7]]), array([[ 8,  9],\n",
       "        [10, 11]]), array([[12, 13],\n",
       "        [14, 15]])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array(range(2**4)).reshape([4, 2, 2])\n",
    "[np.squeeze(a) for a in np.split(x, x.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
